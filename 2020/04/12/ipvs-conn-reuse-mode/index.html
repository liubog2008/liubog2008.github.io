<!DOCTYPE html><html class="theme-next mist use-motion"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"><link rel="stylesheet" href="/css/main.css?v=7.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.0"><link rel="mask-icon" href="/images/logo.svg?v=7.0.0" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"7.0.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="Background之前发现 coredns rolling update 的时候会出现丢包的情况，一步步顺藤摸瓜最后将问题定位到了 ipvs 的实现上，然后发现 ipvs 其实有不少令人窒息的操作，记录一下以作纪念。从 rolling update 说起rolling update 作为 k8s 早期的核心功能，可以说是大杀器一般的存在，极大的保障了升级上线时的服务质量。然而这次丢包让我深入到"><meta name="keywords" content="kernel,ipvs"><meta property="og:type" content="article"><meta property="og:title" content="rolling update 时出错原因分析"><meta property="og:url" content="https://liubog2008.github.io/2020/04/12/ipvs-conn-reuse-mode/index.html"><meta property="og:site_name" content="Blog"><meta property="og:description" content="Background之前发现 coredns rolling update 的时候会出现丢包的情况，一步步顺藤摸瓜最后将问题定位到了 ipvs 的实现上，然后发现 ipvs 其实有不少令人窒息的操作，记录一下以作纪念。从 rolling update 说起rolling update 作为 k8s 早期的核心功能，可以说是大杀器一般的存在，极大的保障了升级上线时的服务质量。然而这次丢包让我深入到"><meta property="og:locale" content="default"><meta property="og:updated_time" content="2020-04-12T11:12:23.658Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="rolling update 时出错原因分析"><meta name="twitter:description" content="Background之前发现 coredns rolling update 的时候会出现丢包的情况，一步步顺藤摸瓜最后将问题定位到了 ipvs 的实现上，然后发现 ipvs 其实有不少令人窒息的操作，记录一下以作纪念。从 rolling update 说起rolling update 作为 k8s 早期的核心功能，可以说是大杀器一般的存在，极大的保障了升级上线时的服务质量。然而这次丢包让我深入到"><link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml"><link rel="canonical" href="https://liubog2008.github.io/2020/04/12/ipvs-conn-reuse-mode/"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>rolling update 时出错原因分析 | Blog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="default"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Blog</span> <span class="logo-line-after"><i></i></span></a></div></div><div class="site-nav-toggle"><button aria-label="Toggle navigation bar"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://liubog2008.github.io/2020/04/12/ipvs-conn-reuse-mode/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="liubog2008"><meta itemprop="description" content="Recording"><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">rolling update 时出错原因分析</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2020-04-12 19:07:21 / Modified: 19:12:23" itemprop="dateCreated datePublished" datetime="2020-04-12T19:07:21+08:00">2020-04-12</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/kernel/" itemprop="url" rel="index"><span itemprop="name">kernel</span></a></span> , <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/kernel/ipvs/" itemprop="url" rel="index"><span itemprop="name">ipvs</span></a></span> </span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Comments: </span><a href="/2020/04/12/ipvs-conn-reuse-mode/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/04/12/ipvs-conn-reuse-mode/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>之前发现 coredns rolling update 的时候会出现丢包的情况，一步步顺藤摸瓜最后将问题定位到了 ipvs 的实现上，然后发现 ipvs 其实有不少令人窒息的操作，记录一下以作纪念。</p><h3 id="从-rolling-update-说起"><a href="#从-rolling-update-说起" class="headerlink" title="从 rolling update 说起"></a>从 rolling update 说起</h3><p>rolling update 作为 k8s 早期的核心功能，可以说是大杀器一般的存在，极大的保障了升级上线时的服务质量。然而这次丢包让我深入到 rolling update 的核心链路，才发现事情远不是如此简单。</p><p>rolling update 简单来说就是对多个副本的服务来说，一台一台的重启副本来达到更新的目的，以此来做到最小化甚至消除更新对服务的影响。</p><p>而为了做到 rolling update 对服务无影响，需要服务提供 graceful shutdown 的功能，并和 k8s 的机制相配合。</p><p>什么是 graceful shutdown 呢？简单来说就是服务端在收到关闭的信号后，不立刻关闭程序，而是不再接收请求，并等待现有的所有连接关闭后再关闭程序。</p><p>以上两个功能相配合，就能做到满足一定条件的客户端不再感知到后端服务的更新。</p><a id="more"></a><h3 id="coredns-丢包"><a href="#coredns-丢包" class="headerlink" title="coredns 丢包"></a>coredns 丢包</h3><p>当然了，理想总是美好的，但现实是配置了以上两个功能的 coredns 更新的时候丢包了。</p><p>整个问题查找的路径大概如下</p><ol><li>发现 coredns 更新时丢包了</li><li>发现 coredns 的某个副本已经关闭后依旧有包发向该副本</li><li>怀疑是 kube-proxy sync 规则到 ipvs 太慢的原因，验证后发现不是</li><li>开始怀疑是 ipvs 的原因，进一步发现 ipvs 会 cache 连接</li><li>确定了问题出现在 ipvs 连接复用上</li></ol><h3 id="ipvs-连接复用"><a href="#ipvs-连接复用" class="headerlink" title="ipvs 连接复用"></a>ipvs 连接复用</h3><p>简单来说，ipvs(NAT) 会 cache 连接，并且依据 ip 包的 source 来复用到 server 的连接，这个功能主要是用于提高 ipvs 的性能</p><p>然而在 rolling update 的时候，ipvs 将即将关闭的 server 的 weight 置为 0 后问题出现了 —— <code>新的连接 hit cache，继续被调度到了这个即将关闭的 server，然后刷新了cache 的过期时间</code></p><h3 id="conn-reuse-mode"><a href="#conn-reuse-mode" class="headerlink" title="conn_reuse_mode"></a>conn_reuse_mode</h3><p>问题出在 ipvs 的内核参数 <code>conn_reuse_mode</code> 上，以下是内核关于该参数的文档</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">conn_reuse_mode - INTEGER</span><br><span class="line">	1 - default</span><br><span class="line"></span><br><span class="line">	Controls how ipvs will deal with connections that are detected</span><br><span class="line">	port reuse. It is a bitmap, with the values being:</span><br><span class="line"></span><br><span class="line">	0: disable any special handling on port reuse. The new</span><br><span class="line">	connection will be delivered to the same real server that was</span><br><span class="line">	servicing the previous connection. This will effectively</span><br><span class="line">	disable expire_nodest_conn.</span><br><span class="line"></span><br><span class="line">	bit 1: enable rescheduling of new connections when it is safe.</span><br><span class="line">	That is, whenever expire_nodest_conn and for TCP sockets, when</span><br><span class="line">	the connection is in TIME_WAIT state (which is only possible if</span><br><span class="line">	you use NAT mode).</span><br><span class="line"></span><br><span class="line">	bit 2: it is bit 1 plus, for TCP connections, when connections</span><br><span class="line">	are in FIN_WAIT state, as this is the last state seen by load</span><br><span class="line">	balancer in Direct Routing mode. This bit helps on adding new</span><br><span class="line">	real servers to a very busy cluster.</span><br></pre></td></tr></table></figure><p>可以发现，在 <code>conn_reuse_mode</code> 为 0 的时候，新连接总是会根据 cache 调度到之前的 real server 上</p><p>而在 <code>conn_reuse_mode</code> 为 1 的时候，在 real server 的 weight 为 0 或者 real server 被删除的时候新的链接会被重新调度</p><p>那么为什么 kube-proxy 要把默认值改为 0 呢？见 <a href="https://github.com/kubernetes/kubernetes/issues/70747" target="_blank" rel="noopener">issue 70747</a> 可以知道是因为改为 0 后性能上 可以提高不少</p><p>然而这一改却带来了这个问题</p><h3 id="drop-SYN-？"><a href="#drop-SYN-？" class="headerlink" title="drop SYN ？"></a>drop SYN ？</h3><p>那么改回来问题就解决了么？事实上没有，因为 <code>conn_reuse_mode</code> 为 1 时同样会遇到一个奇怪的问题</p><p>ipvs expire 连接调用了 <code>ip_vs_conn_expire_now</code> 的函数，这个函数是将 expire 连接的计时器设置为 0 来触发 expire 的函数</p><p>然而新连接的建立会刷新 nf_conntrack 信息，然后 expire 函数的触发会误删 nf_contrack 的信息，整个流程大概如下</p><ol><li>TCP 连接被关闭</li><li>新连接开始建立，conntrack 被替代</li><li>ipvs 重新调度新连接</li><li>ipvs conn expire 时间置 0</li><li>ipvs expire 被调用</li><li>conntrack 被清理 (清理的是新连接的 conntrack)</li></ol><p>而这个 bug 的修复方法非常的简单粗暴，就是将新连接的 SYN 包丢弃，然后依赖 TCP 重发机制来保证正确性</p><p>这样做的结果很简单，某些请求的 rt 至少要延长到 1s (重发 SYN 的间隔)</p><p>以下是修复该问题的 commit message</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">commit f719e3754ee2f7275437e61a6afd520181fdd43b</span><br><span class="line">Author: Julian Anastasov &lt;ja@ssi.bg&gt;</span><br><span class="line">Date:   Sat Mar 5 15:03:22 2016 +0200</span><br><span class="line"></span><br><span class="line">    ipvs: drop first packet to redirect conntrack</span><br><span class="line"></span><br><span class="line">    Jiri Bohac is reporting for a problem where the attempt</span><br><span class="line">    to reschedule existing connection to another real server</span><br><span class="line">    needs proper redirect for the conntrack used by the IPVS</span><br><span class="line">    connection. For example, when IPVS connection is created</span><br><span class="line">    to NAT-ed real server we alter the reply direction of</span><br><span class="line">    conntrack. If we later decide to select different real</span><br><span class="line">    server we can not alter again the conntrack. And if we</span><br><span class="line">    expire the old connection, the new connection is left</span><br><span class="line">    without conntrack.</span><br><span class="line"></span><br><span class="line">    So, the only way to redirect both the IPVS connection and</span><br><span class="line">    the Netfilter&apos;s conntrack is to drop the SYN packet that</span><br><span class="line">    hits existing connection, to wait for the next jiffie</span><br><span class="line">    to expire the old connection and its conntrack and to rely</span><br><span class="line">    on client&apos;s retransmission to create new connection as</span><br><span class="line">    usually.</span><br><span class="line"></span><br><span class="line">    Jiri Bohac provided a fix that drops all SYNs on rescheduling,</span><br><span class="line">    I extended his patch to do such drops only for connections</span><br><span class="line">    that use conntrack. Here is the original report from Jiri Bohac:</span><br><span class="line"></span><br><span class="line">    Since commit dc7b3eb900aa (&quot;ipvs: Fix reuse connection if real server</span><br><span class="line">    is dead&quot;), new connections to dead servers are redistributed</span><br><span class="line">    immediately to new servers.  The old connection is expired using</span><br><span class="line">    ip_vs_conn_expire_now() which sets the connection timer to expire</span><br><span class="line">    immediately.</span><br><span class="line"></span><br><span class="line">    However, before the timer callback, ip_vs_conn_expire(), is run</span><br><span class="line">    to clean the connection&apos;s conntrack entry, the new redistributed</span><br><span class="line">    connection may already be established and its conntrack removed</span><br><span class="line">    instead.</span><br><span class="line"></span><br><span class="line">    Fix this by dropping the first packet of the new connection</span><br><span class="line">    instead, like we do when the destination server is not available.</span><br><span class="line">    The timer will have deleted the old conntrack entry long before</span><br><span class="line">    the first packet of the new connection is retransmitted.</span><br><span class="line"></span><br><span class="line">    Fixes: dc7b3eb900aa (&quot;ipvs: Fix reuse connection if real server is dead&quot;)</span><br><span class="line">    Signed-off-by: Jiri Bohac &lt;jbohac@suse.cz&gt;</span><br><span class="line">    Signed-off-by: Julian Anastasov &lt;ja@ssi.bg&gt;</span><br><span class="line">    Signed-off-by: Simon Horman &lt;horms@verge.net.au&gt;</span><br></pre></td></tr></table></figure><p>见 <a href="https://github.com/kubernetes/kubernetes/issues/81775" target="_blank" rel="noopener">issue 81775</a> 关于该问题的讨论</p><h3 id="等等问题不是出在-UDP-么"><a href="#等等问题不是出在-UDP-么" class="headerlink" title="等等问题不是出在 UDP 么"></a>等等问题不是出在 UDP 么</h3><p>眼尖的人已经发现了，问题最初来自于 DNS 请求，而 DNS 请求往往是 UDP 协议。然而上面一堆都是 TCP 连接的问题(TCP 的问题关我 UDP 什么事情)。</p><p>于是去源代码找答案，以下是关键代码一，也就是 <code>conn_reuse_mode</code> 生效的部分</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">conn_reuse_mode = sysctl_conn_reuse_mode(ipvs);</span><br><span class="line">if (conn_reuse_mode &amp;&amp; !iph.fragoffs &amp;&amp; is_new_conn(skb, &amp;iph) &amp;&amp; cp) &#123;</span><br><span class="line">	bool uses_ct = false, resched = false;</span><br><span class="line"></span><br><span class="line">	if (unlikely(sysctl_expire_nodest_conn(ipvs)) &amp;&amp; cp-&gt;dest &amp;&amp;</span><br><span class="line">	    unlikely(!atomic_read(&amp;cp-&gt;dest-&gt;weight))) &#123;</span><br><span class="line">		resched = true;</span><br><span class="line">		uses_ct = ip_vs_conn_uses_conntrack(cp, skb);</span><br><span class="line">	&#125; else if (is_new_conn_expected(cp, conn_reuse_mode)) &#123;</span><br><span class="line">		uses_ct = ip_vs_conn_uses_conntrack(cp, skb);</span><br><span class="line">		if (!atomic_read(&amp;cp-&gt;n_control)) &#123;</span><br><span class="line">			resched = true;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			/* Do not reschedule controlling connection</span><br><span class="line">			 * that uses conntrack while it is still</span><br><span class="line">			 * referenced by controlled connection(s).</span><br><span class="line">			 */</span><br><span class="line">			resched = !uses_ct;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (resched) &#123;</span><br><span class="line">		if (!atomic_read(&amp;cp-&gt;n_control))</span><br><span class="line">			ip_vs_conn_expire_now(cp);</span><br><span class="line">		__ip_vs_conn_put(cp);</span><br><span class="line">		if (uses_ct)</span><br><span class="line">			return NF_DROP;</span><br><span class="line">		cp = NULL;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到 reschedule 至少需要满足以下条件</p><ol><li>conn_reuse_mode 不为 0</li><li>is_new_conn(skb, &amp;iph) 需要为 true</li></ol><p>而 is_new_conn 如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">static inline bool is_new_conn(const struct sk_buff *skb,</span><br><span class="line">			       struct ip_vs_iphdr *iph)</span><br><span class="line">&#123;</span><br><span class="line">	switch (iph-&gt;protocol) &#123;</span><br><span class="line">	case IPPROTO_TCP: &#123;</span><br><span class="line">		struct tcphdr _tcph, *th;</span><br><span class="line"></span><br><span class="line">		th = skb_header_pointer(skb, iph-&gt;len, sizeof(_tcph), &amp;_tcph);</span><br><span class="line">		if (th == NULL)</span><br><span class="line">			return false;</span><br><span class="line">		return th-&gt;syn;</span><br><span class="line">	&#125;</span><br><span class="line">	case IPPROTO_SCTP: &#123;</span><br><span class="line">		struct sctp_chunkhdr *sch, schunk;</span><br><span class="line"></span><br><span class="line">		sch = skb_header_pointer(skb, iph-&gt;len + sizeof(struct sctphdr),</span><br><span class="line">					 sizeof(schunk), &amp;schunk);</span><br><span class="line">		if (sch == NULL)</span><br><span class="line">			return false;</span><br><span class="line">		return sch-&gt;type == SCTP_CID_INIT;</span><br><span class="line">	&#125;</span><br><span class="line">	default:</span><br><span class="line">		return false;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>显然该函数只处理了 TCP 和 STCP 两种协议，其他协议都默认返回 false，因此 <code>conn_reuse_mode</code> 对 UDP 并没有任何用处。</p><p>然后看到了第二段关键代码如下，即在 real server 真正被删除后的情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">if (cp-&gt;dest &amp;&amp; !(cp-&gt;dest-&gt;flags &amp; IP_VS_DEST_F_AVAILABLE)) &#123;</span><br><span class="line">	/* the destination server is not available */</span><br><span class="line"></span><br><span class="line">	__u32 flags = cp-&gt;flags;</span><br><span class="line"></span><br><span class="line">	/* when timer already started, silently drop the packet.*/</span><br><span class="line">	if (timer_pending(&amp;cp-&gt;timer))</span><br><span class="line">		__ip_vs_conn_put(cp);</span><br><span class="line">	else</span><br><span class="line">		ip_vs_conn_put(cp);</span><br><span class="line"></span><br><span class="line">	if (sysctl_expire_nodest_conn(ipvs) &amp;&amp;</span><br><span class="line">	    !(flags &amp; IP_VS_CONN_F_ONE_PACKET)) &#123;</span><br><span class="line">		/* try to expire the connection immediately */</span><br><span class="line">		ip_vs_conn_expire_now(cp);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return NF_DROP;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以发现，UDP 包并不会在 real server 的 weight 为 0 时重新调度，如果 client 端请求量巨大，可能会不断 hit cache 直到 real server 被移除，然后导致大量请求被 Drop</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>总的来说 ipvs 里还有不少问题，而这些问题会导致 rolling update 时总有几个连接超时或出错</p><p>对于 TCP 连接而言，<code>conn_reuse_mode</code> 为 0 时部分连接可能会丢失，而为 1 时部分连接可能会出现 SYN 包重发导致的超时</p><p>而对于 UDP 连接而言，当请求量巨大时，几乎可以说 rolling update 总是会丢包了，尤其是对 DNS 而言</p><p>感受最深的是内核中竟然会用 Drop SYN 包这种骚操作来 fix 问题。。。服务的无感知更新真是太艰难了</p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/kernel/" rel="tag"># kernel</a> <a href="/tags/ipvs/" rel="tag"># ipvs</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/04/08/kubernetes/kubelet-1/" rel="next" title="Kubernetes 源码分析之 kubelet(一)"><i class="fa fa-chevron-left"></i> Kubernetes 源码分析之 kubelet(一)</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2020/11/30/kubernetes/kubelet-2/" rel="prev" title="Kubernetes 源码分析之 kubelet(二)">Kubernetes 源码分析之 kubelet(二) <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div><div class="comments" id="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Overview</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">liubog2008</p><p class="site-description motion-element" itemprop="description">Recording</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">6</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">8</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">9</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Background"><span class="nav-number">1.</span> <span class="nav-text">Background</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#从-rolling-update-说起"><span class="nav-number">1.1.</span> <span class="nav-text">从 rolling update 说起</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coredns-丢包"><span class="nav-number">1.2.</span> <span class="nav-text">coredns 丢包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ipvs-连接复用"><span class="nav-number">1.3.</span> <span class="nav-text">ipvs 连接复用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#conn-reuse-mode"><span class="nav-number">1.4.</span> <span class="nav-text">conn_reuse_mode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#drop-SYN-？"><span class="nav-number">1.5.</span> <span class="nav-text">drop SYN ？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#等等问题不是出在-UDP-么"><span class="nav-number">1.6.</span> <span class="nav-text">等等问题不是出在 UDP 么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">1.7.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div></div></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love" id="animate"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">liubog2008</span></div><div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div><span class="post-meta-divider">|</span><div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.0.0</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/js/src/utils.js?v=7.0.0"></script><script src="/js/src/motion.js?v=7.0.0"></script><script src="/js/src/schemes/muse.js?v=7.0.0"></script><script src="/js/src/scrollspy.js?v=7.0.0"></script><script src="/js/src/post-details.js?v=7.0.0"></script><script src="/js/src/bootstrap.js?v=7.0.0"></script><script id="dsq-count-scr" src="https://blog-t0ykepqjjj.disqus.com/count.js" async></script><script>var disqus_config=function(){this.page.url="https://liubog2008.github.io/2020/04/12/ipvs-conn-reuse-mode/",this.page.identifier="2020/04/12/ipvs-conn-reuse-mode/",this.page.title="rolling update 时出错原因分析"};function loadComments(){var e=document,t=e.createElement("script");t.src="https://blog-t0ykepqjjj.disqus.com/embed.js",t.setAttribute("data-timestamp",""+ +new Date),(e.head||e.body).appendChild(t)}loadComments()</script></body></html>